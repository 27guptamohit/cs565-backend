{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from dotenv import dotenv_values\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from music_symbol import MusicSymbol\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from response_metrics import ResponseMetrics\n",
    "import sys\n",
    "\n",
    "# Importing from parent directory\n",
    "curr_file = Path(os.path.abspath('')).resolve()\n",
    "sys.path.append(str(curr_file.parent))\n",
    "from scripts.backup_data import main as load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbing data from DB\n",
      "Wrote users backup to ../backups/live/20230506-182133_users_backup.json\n",
      "Wrote sheets backup to ../backups/live/20230506-182133_sheets_backup.json\n",
      "Wrote measures backup to ../backups/live/20230506-182133_measures_backup.json\n"
     ]
    }
   ],
   "source": [
    "backup_prefix = \"testing/20230504-145732\"\n",
    "# Set to None to pull from the database live\n",
    "backup_prefix = None\n",
    "\n",
    "if backup_prefix is None:\n",
    "    config = dotenv_values(\"../.env\")\n",
    "    ENDPOINT = config[\"ENDPOINT\"]\n",
    "\n",
    "    print(\"Grabbing data from DB\")\n",
    "    users_data, sheets_data, measures_data = load_data(ENDPOINT, \"../backups/live\")\n",
    "else:\n",
    "    print(\"Loading from backups\")\n",
    "    with open(f\"../backups/{backup_prefix}_users_backup.json\", \"r\") as file:\n",
    "        users_data = json.load(file)\n",
    "\n",
    "    with open(f\"../backups/{backup_prefix}_sheets_backup.json\", \"r\") as file:\n",
    "        sheets_data = json.load(file)\n",
    "\n",
    "    with open(f\"../backups/{backup_prefix}_measures_backup.json\", \"r\") as file:\n",
    "        measures_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_data: dict[str, ResponseMetrics] = {}\n",
    "crowdsourced_response_data: dict[str, ResponseMetrics] = {}\n",
    "crowdsourced_agreement_count: int = 0\n",
    "\n",
    "for measure in measures_data:\n",
    "    gold_symbols: list[MusicSymbol] = []\n",
    "    for symbol in measure[\"goldSymbols\"]:\n",
    "        gold_symbols.append(MusicSymbol(**symbol))\n",
    "    \n",
    "    while (len(gold_symbols) < 4):\n",
    "        gold_symbols.append(None)\n",
    "    \n",
    "    response_counter = Counter()\n",
    "\n",
    "    for response in measure[\"responses\"]:\n",
    "        response_id = response[\"_id\"]\n",
    "        user_symbols: list[MusicSymbol] = []\n",
    "        for symbol in response[\"symbols\"]:\n",
    "            user_symbols.append(MusicSymbol(**symbol))\n",
    "        \n",
    "        while (len(user_symbols) < 4):\n",
    "            user_symbols.append(None)\n",
    "\n",
    "        response_counter[tuple(user_symbols)] += 1\n",
    "    \n",
    "        response_data[response_id] = ResponseMetrics(user_symbols, gold_symbols)\n",
    "    \n",
    "    # no responses, skip\n",
    "    if len(response_counter) == 0:\n",
    "        continue\n",
    "\n",
    "    measure_id = measure[\"_id\"]\n",
    "    crowdsourced_symbols, primary_count = response_counter.most_common(1)[0]\n",
    "\n",
    "    if len(response_counter) >= 2:\n",
    "        # get the count of the second most frequent item\n",
    "        second_count = response_counter.most_common(2)[1][1]\n",
    "\n",
    "        # tie, no crowdsourced answer\n",
    "        if primary_count > second_count:\n",
    "            crowdsourced_agreement_count += 1\n",
    "    else:\n",
    "        crowdsourced_agreement_count += 1\n",
    "    \n",
    "    crowdsourced_response_data[measure_id] = ResponseMetrics(list(crowdsourced_symbols), gold_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(datapoints: list[int], title: str) -> None:\n",
    "    print(title)\n",
    "    print(\"-\" * len(title))\n",
    "    print(f\"{'Count:':<7} {len(datapoints):.4f}\")\n",
    "    print(f\"{'Min:':<7} {np.min(datapoints):.4f}\")\n",
    "    print(f\"{'Max:':<7} {np.max(datapoints):.4f}\")\n",
    "    print(f\"{'Median:':<7} {np.median(datapoints):.4f}\")\n",
    "    print(f\"{'Mean:':<7} {np.mean(datapoints):.4f}\")\n",
    "    print(f\"{'Stddev:':<7} {np.std(datapoints):.4f}\")\n",
    "\n",
    "def calculate_response_statistics(response_metrics_map: dict[str, ResponseMetrics], key_group: list[str], title: str, units: str=\"responses\", print_results: bool=True):\n",
    "    symbol_num_match_count = 0\n",
    "    name_match_count = 0\n",
    "    pitch_match_count = 0\n",
    "    exact_match_count = 0\n",
    "    \n",
    "    pitch_count = 0\n",
    "    total_count = len(key_group)\n",
    "    for key in key_group:\n",
    "        if response_metrics_map[key].full_symbol_count_match():\n",
    "            symbol_num_match_count += 1\n",
    "\n",
    "        if response_metrics_map[key].full_name_match():\n",
    "            name_match_count += 1\n",
    "        \n",
    "        pitch_match = response_metrics_map[key].full_pitch_match()\n",
    "        if pitch_match is not None:\n",
    "            pitch_count += 1\n",
    "\n",
    "            if pitch_match:\n",
    "                pitch_match_count += 1\n",
    "        \n",
    "        if response_metrics_map[key].full_exact_match():\n",
    "            exact_match_count += 1\n",
    "    \n",
    "    symbol_num_match_accuracy = symbol_num_match_count / total_count\n",
    "    name_match_accuracy = name_match_count / total_count\n",
    "    pitch_match_accuracy = pitch_match_count / pitch_count\n",
    "    exact_match_accuracy = exact_match_count / total_count\n",
    "\n",
    "    if print_results:\n",
    "        print(title)\n",
    "        print(\"-\" * len(title))\n",
    "        print(f\"{total_count} {units}, {pitch_count} {units} with pitch content\")\n",
    "        print(f\"{'Symbol count accuracy:':<35} {symbol_num_match_count}/{total_count} {units} = {symbol_num_match_accuracy * 100:.4f}%\")\n",
    "        print(f\"{'Symbol identification accuracy:':<35} {name_match_count}/{total_count} {units} = {name_match_accuracy * 100:.4f}%\")\n",
    "        print(f\"{'Symbol pitch accuracy:':<35} {pitch_match_count}/{pitch_count} {units} = {pitch_match_accuracy * 100:.4f}%\")\n",
    "        print(f\"{'Symbol exact match accuracy:':<35} {exact_match_count}/{total_count} {units} = {exact_match_accuracy * 100:.4f}%\")\n",
    "    \n",
    "    return symbol_num_match_accuracy, name_match_accuracy, pitch_match_accuracy, exact_match_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_by_experience: dict[str, list[str]] = dict()\n",
    "\n",
    "for user in users_data:\n",
    "    experience = user[\"experience\"]\n",
    "    user_id = user[\"_id\"]\n",
    "    if experience not in users_by_experience:\n",
    "        users_by_experience[experience] = []\n",
    "\n",
    "    users_by_experience[experience].append(user_id)\n",
    "\n",
    "responses_by_user: dict[str, list] = dict()\n",
    "unknown_count = 0\n",
    "for measure in measures_data:\n",
    "    for response in measure[\"responses\"]:\n",
    "        if \"userId\" not in response:\n",
    "            # print(\"userId not found\")\n",
    "            # TODO: handle this?\n",
    "            unknown_count += 1\n",
    "            continue\n",
    "            # user_id = f\"unknown{unknown_count}\"\n",
    "        else:\n",
    "            user_id = response[\"userId\"]\n",
    "        response_id = response[\"_id\"]\n",
    "\n",
    "        if user_id not in responses_by_user:\n",
    "            responses_by_user[user_id] = []\n",
    "        \n",
    "        responses_by_user[user_id].append(response_id)\n",
    "\n",
    "# print(len(responses_by_user))\n",
    "# print(unknown_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Level Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of users: 69\n",
      "Number of users with experience level: Zero experience: 47\n",
      "Number of users with experience level: Some previous experience: 18\n",
      "Number of users with experience level: Actively work with sheet music: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of users: {len(users_data)}\")\n",
    "\n",
    "experience_levels = [\"Zero experience\", \"Some previous experience\", \"Actively work with sheet music\"]\n",
    "\n",
    "for experience_level in experience_levels:\n",
    "    print(f\"Number of users with experience level: {experience_level}: {len(users_by_experience[experience_level])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sheets: 3\n",
      "Total number of measures: 48\n",
      "Total number of responses: 434\n",
      "Responses from users with experience 'Zero experience': 290\n",
      "Responses from users with experience 'Some previous experience': 125\n",
      "Responses from users with experience 'Actively work with sheet music': 8\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of sheets: {len(sheets_data)}\")\n",
    "print(f\"Total number of measures: {len(measures_data)}\")\n",
    "\n",
    "responses = 0\n",
    "for measure in measures_data:\n",
    "    responses += len(measure[\"responses\"])\n",
    "\n",
    "print(f\"Total number of responses: {responses}\")\n",
    "\n",
    "for experience_level in experience_levels:\n",
    "    response_count = sum([len(responses_by_user[user_id]) for user_id in responses_by_user.keys() if user_id in users_by_experience[experience_level]])\n",
    "    print(f\"Responses from users with experience '{experience_level}': {response_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_counts_per_measure = []\n",
    "# for measure in measures_data:\n",
    "#     response_counts_per_measure.append(len(measure[\"responses\"]))\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# vals, counts = np.unique(response_counts_per_measure, return_counts=True)\n",
    "# width = np.min(np.diff(np.unique(vals))) * 0.5\n",
    "# ax.bar(vals, counts, width=width, edgecolor=\"black\")\n",
    "# ax.set_xlabel(\"# of Responses for a Given Measure\")\n",
    "# ax.set_ylabel(\"Frequency\")\n",
    "# ax.set_title(f\"Responses Histogram\")\n",
    "\n",
    "# max_occurrences = np.max(counts)\n",
    "# ax.set_ylim((0, max_occurrences * 1.25))\n",
    "# mean = np.mean(response_counts_per_measure)\n",
    "# stddev = np.std(response_counts_per_measure)\n",
    "# ax.axvline(mean, linestyle=\"dashed\")\n",
    "# ax.text(mean + 0.05 * (ax.get_xlim()[1] - ax.get_xlim()[0]), ax.get_ylim()[1] * 0.95, f\"Mean: {mean:.2f}\")\n",
    "# ax.text(mean + 0.05 * (ax.get_xlim()[1] - ax.get_xlim()[0]), ax.get_ylim()[1] * 0.9, f\"Stddev: {stddev:.4f}\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Counts per Measure\n",
      "---------------------------\n",
      "Count:  48.0000\n",
      "Min:    8.0000\n",
      "Max:    10.0000\n",
      "Median: 9.0000\n",
      "Mean:   9.0417\n",
      "Stddev: 0.2857\n"
     ]
    }
   ],
   "source": [
    "response_counts_per_measure = [len(measure[\"responses\"]) for measure in measures_data]\n",
    "calculate_statistics(response_counts_per_measure, \"Response Counts per Measure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Counts per User\n",
      "------------------------\n",
      "Count:  47.0000\n",
      "Min:    1.0000\n",
      "Max:    21.0000\n",
      "Median: 8.0000\n",
      "Mean:   9.0000\n",
      "Stddev: 3.7756\n"
     ]
    }
   ],
   "source": [
    "response_counts_per_user = [len(val) for val in responses_by_user.values()]\n",
    "calculate_statistics(response_counts_per_user, \"Response Counts per User\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All response data metrics\n",
      "-------------------------\n",
      "434 responses, 373 responses with pitch content\n",
      "Symbol count accuracy:              408/434 responses = 94.0092%\n",
      "Symbol identification accuracy:     375/434 responses = 86.4055%\n",
      "Symbol pitch accuracy:              249/373 responses = 66.7560%\n",
      "Symbol exact match accuracy:        294/434 responses = 67.7419%\n"
     ]
    }
   ],
   "source": [
    "calculate_response_statistics(response_data, response_data.keys(), title=\"All response data metrics\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crowdsourced response data metrics\n",
      "----------------------------------\n",
      "48 measures, 41 measures with pitch content\n",
      "Symbol count accuracy:              48/48 measures = 100.0000%\n",
      "Symbol identification accuracy:     47/48 measures = 97.9167%\n",
      "Symbol pitch accuracy:              39/41 measures = 95.1220%\n",
      "Symbol exact match accuracy:        46/48 measures = 95.8333%\n",
      "Crowd conclusively agreed on:       46/48 measures = 95.8333%\n"
     ]
    }
   ],
   "source": [
    "calculate_response_statistics(crowdsourced_response_data, crowdsourced_response_data.keys(), title=\"Crowdsourced response data metrics\", units=\"measures\")\n",
    "print(f\"{'Crowd conclusively agreed on:':<35} {crowdsourced_agreement_count}/{len(crowdsourced_response_data)} measures = {crowdsourced_agreement_count / len(crowdsourced_response_data) * 100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response data metrics for experience level 'Zero experience'\n",
      "------------------------------------------------------------\n",
      "290 responses, 253 responses with pitch content\n",
      "Symbol count accuracy:              273/290 responses = 94.1379%\n",
      "Symbol identification accuracy:     247/290 responses = 85.1724%\n",
      "Symbol pitch accuracy:              163/253 responses = 64.4269%\n",
      "Symbol exact match accuracy:        186/290 responses = 64.1379%\n",
      "\n",
      "Response data metrics for experience level 'Some previous experience'\n",
      "---------------------------------------------------------------------\n",
      "125 responses, 104 responses with pitch content\n",
      "Symbol count accuracy:              116/125 responses = 92.8000%\n",
      "Symbol identification accuracy:     109/125 responses = 87.2000%\n",
      "Symbol pitch accuracy:              70/104 responses = 67.3077%\n",
      "Symbol exact match accuracy:        89/125 responses = 71.2000%\n",
      "\n",
      "Response data metrics for experience level 'Actively work with sheet music'\n",
      "---------------------------------------------------------------------------\n",
      "8 responses, 7 responses with pitch content\n",
      "Symbol count accuracy:              8/8 responses = 100.0000%\n",
      "Symbol identification accuracy:     8/8 responses = 100.0000%\n",
      "Symbol pitch accuracy:              7/7 responses = 100.0000%\n",
      "Symbol exact match accuracy:        8/8 responses = 100.0000%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for experience_level in experience_levels:\n",
    "    response_ids = [responses_by_user[user_id] for user_id in responses_by_user.keys() if user_id in users_by_experience[experience_level]]\n",
    "    response_ids = [id for sublist in response_ids for id in sublist]\n",
    "    calculate_response_statistics(response_data, response_ids, title=f\"Response data metrics for experience level '{experience_level}'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Accuracy per User\n",
      "-----------------------\n",
      "Count:  47.0000\n",
      "Min:    0.0000\n",
      "Max:    1.0000\n",
      "Median: 1.0000\n",
      "Mean:   0.8973\n",
      "Stddev: 0.2412\n",
      "Identification Accuracy per User\n",
      "--------------------------------\n",
      "Count:  47.0000\n",
      "Min:    0.0000\n",
      "Max:    1.0000\n",
      "Median: 1.0000\n",
      "Mean:   0.8085\n",
      "Stddev: 0.3146\n",
      "Pitch Accuracy per User\n",
      "-----------------------\n",
      "Count:  47.0000\n",
      "Min:    0.0000\n",
      "Max:    1.0000\n",
      "Median: 0.8000\n",
      "Mean:   0.6008\n",
      "Stddev: 0.4064\n",
      "Exact Match Accuracy per User\n",
      "-----------------------------\n",
      "Count:  47.0000\n",
      "Min:    0.0000\n",
      "Max:    1.0000\n",
      "Median: 0.7000\n",
      "Mean:   0.6113\n",
      "Stddev: 0.3700\n"
     ]
    }
   ],
   "source": [
    "user_count_accuracy = []\n",
    "user_identification_accuracy = []\n",
    "user_pitch_accuracy = []\n",
    "user_exact_match_accuracy = []\n",
    "\n",
    "for responses in responses_by_user.values():\n",
    "    count_acc, ident_acc, pitch_acc, exact_match_acc = calculate_response_statistics(response_data, responses, title=None, print_results=False)\n",
    "    user_count_accuracy.append(count_acc)\n",
    "    user_identification_accuracy.append(ident_acc)\n",
    "    user_pitch_accuracy.append(pitch_acc)\n",
    "    user_exact_match_accuracy.append(exact_match_acc)\n",
    "\n",
    "calculate_statistics(user_count_accuracy, \"Count Accuracy per User\")\n",
    "calculate_statistics(user_identification_accuracy, \"Identification Accuracy per User\")\n",
    "calculate_statistics(user_pitch_accuracy, \"Pitch Accuracy per User\")\n",
    "calculate_statistics(user_exact_match_accuracy, \"Exact Match Accuracy per User\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
